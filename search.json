[
  {
    "objectID": "showdoc.html",
    "href": "showdoc.html",
    "title": "showdoc",
    "section": "",
    "text": "qual_name\n\nqual_name(obj)\n\nGet the qualified name of obj\n\n\n\n\n\n\n\nShowDocRenderer\n\nShowDocRenderer(sym, disp: bool = True)\n\nShow documentation for sym\n\n\n\n\n\n\n\nBasicMarkdownRenderer\n\nBasicMarkdownRenderer(sym, disp: bool = True)\n\nShow documentation for sym\n\n\n\n\n\n\n\nshow_doc\n\nshow_doc(sym, disp=True, renderer=None)\n\n\n\n\nYou can use show_doc to document apis of functions, classes or methods:\n\n\n\n\nf\n\nf(x: int = 1)\n\nfunc docstring\n\n\n\n\n\n\n\nFoo\n\nFoo(e: int)\n\nThis is the docstring for the init method\n\n\n\n\n\n\n\nFoo.a_method\n\nFoo.a_method(a: list, b: dict, c)\n\nThis is a method\n\n\n\n\n\n\n\nBasicHtmlRenderer\n\nBasicHtmlRenderer(sym, disp: bool = True)\n\nShow documentation for sym\n\n\n\n\n\n\nF\nF(x: int = 1)class docstring\n\n\n\n\n\n\nshowdoc_nm\n\nshowdoc_nm(tree)\n\nGet the fully qualified name for showdoc."
  },
  {
    "objectID": "processors.html",
    "href": "processors.html",
    "title": "processors",
    "section": "",
    "text": "On this page we’ll be using this private helper to process a notebook and return the results, to simplify testing:\n\ndef _run_procs(procs=None, preprocs=None, postprocs=None):\n    nbp = NBProcessor(_test_file, procs, preprocs=preprocs, postprocs=postprocs)\n    nbp.process()\n    return '\\n'.join([str(cell) for cell in nbp.nb.cells])"
  },
  {
    "objectID": "processors.html#cell-processors",
    "href": "processors.html#cell-processors",
    "title": "processors",
    "section": "Cell processors",
    "text": "Cell processors\n\n\n\n\nadd_links\n\nadd_links(cell)\n\nAdd links to markdown cells\n\n\n\n\nres = _run_procs(add_links)\nassert \"[numpy.array](https://numpy.org/doc/stable/reference/generated/numpy.array.html#numpy.array)\" in res\nassert \"[read_nb](https://nbprocess.fast.ai/read#read_nb) but not a link to `foobar`.\" in res\nassert \"A link in a docstring: [write_nb](https://nbprocess.fast.ai/sync#write_nb)\" in res\nassert \"And not a link to <code>dict2nb</code>.\" in res\n\nGets rid of colors that are streamed from standard out, which can interfere with static site generators:\n\n\n\n\nstrip_ansi\n\nstrip_ansi(cell)\n\nStrip Ansi Characters.\n\n\n\n\nres = _run_procs(strip_ansi)\nassert not _re_ansi_escape.findall(res)\n\n\n\n\n\nhide_\n\nhide_(nbp, cell)\n\nHide cell from output\n\n\n\n\nres = _run_procs(hide_)\nassert 'you will not be able to see this cell at all either' not in res\n\n\n\n\n\nhide_line\n\nhide_line(cell)\n\nHide lines of code in code cells with the directive hide_line at the end of a line of code\n\n\n\n\nres = _run_procs(hide_line)\nassert r\"def show():\\n    a = 2\\n    b = 3\" not in res\nassert r\"def show():\\n    a = 2\"                in res\n\n\n\n\n\nfilter_stream_\n\nfilter_stream_(nbp, cell, *words)\n\nRemove output lines containing any of words in cell stream output\n\n\n\n\nres = _run_procs(filter_stream_)\nexp=r\"'A line\\n', 'Another line.\\n'\"\nassert exp in res\n\n\n\n\n\nclean_magics\n\nclean_magics(cell)\n\nA preprocessor to remove cell magic commands\n\n\n\n\nres = _run_procs(clean_magics)\nassert \"%%\" not in res\n\n\n\n\n\nlang_identify\n\nlang_identify(cell)\n\nA preprocessor to identify bash/js/etc cells and mark them appropriately\n\n\n\nWhen we issue a shell command in a notebook with !, we need to change the code-fence from python to bash and remove the !:\n\nres = _run_procs(lang_identify)\nassert \"'language': 'bash'\" in res\n\n\n\n\n\nrm_header_dash\n\nrm_header_dash(cell)\n\nRemove headings that end with a dash -\n\n\n\n\nres = _run_procs(rm_header_dash)\nassert 'some words' in res\nassert 'A heading to Hide' not in res\nassert 'Yet another heading to hide' not in res\n\n\n\n\n\nrm_export\n\nrm_export(cell)\n\nRemove cells that are exported or hidden\n\n\n\n\nres = _run_procs(rm_export)\nassert 'dontshow' not in res\n\n\n\n\n\nexec_show_docs\n\nexec_show_docs()\n\nExecute cells needed for show_docs output, including exported cells and imports\n\n\n\n\nres = _run_procs(exec_show_docs)\nassert res\n\n\n\n\n\nclean_show_doc\n\nclean_show_doc(cell)\n\nRemove ShowDoc input cells"
  },
  {
    "objectID": "processors.html#notebook-preprocessors",
    "href": "processors.html#notebook-preprocessors",
    "title": "processors",
    "section": "Notebook preprocessors",
    "text": "Notebook preprocessors\n\n\n\n\ninsert_warning\n\ninsert_warning(nb)\n\nInsert Autogenerated Warning Into Notebook after the first cell.\n\n\n\nThis preprocessor inserts a warning in the markdown destination that the file is autogenerated. This warning is inserted in the second cell so we do not interfere with front matter.\n\nres = _run_procs(preprocs=[insert_warning])\nassert \"<!-- WARNING: THIS FILE WAS AUTOGENERATED!\" in res\n\n\nL('foo', None, 'a').filter(lambda x:x == 1)\n_tstre = re.compile('a')\n\n\n\n\n\nadd_show_docs\n\nadd_show_docs(nb)\n\nAdd show_doc cells after exported cells, unless they are already documented\n\n\n\n\nres = _run_procs(preprocs=add_show_docs)\nassert \"show_doc(some_func)'\" in res\nassert \"show_doc(and_another)'\" in res\nassert \"show_doc(another_func)'\" not in res"
  },
  {
    "objectID": "processors.html#notebook-postprocessors",
    "href": "processors.html#notebook-postprocessors",
    "title": "processors",
    "section": "Notebook postprocessors",
    "text": "Notebook postprocessors\n\n\n\n\nis_frontmatter\n\nis_frontmatter(nb)\n\n\n\n\n\n_testnb = read_nb('../tests/docs_test.ipynb')\ntest_eq(_default_exp(_testnb), 'foobar')\n\n\n\n\n\nnb_fmdict\n\nnb_fmdict(nb, remove=True)\n\nInfer the front matter from a notebook’s markdown formatting\n\n\n\n\n_testnb = read_nb('../tests/docs_test.ipynb')\n_res = nb_fmdict(_testnb)\ntest_eq(_res, \n       {'key1': ' value1', 'key2': ' value2', \n        'categories': ' [c1, c2]', 'title': 'a title', \n        'description': 'A description'})\n\n\n\n\n\nconstruct_fm\n\nconstruct_fm(fmdict: dict, keys=['title', 'description', 'author', 'image', 'categories', 'output-file', 'aliases'])\n\nconstruct front matter from a dictionary, but only for keys\n\n\n\n\n_testdict = nb_fmdict(read_nb('../tests/docs_test.ipynb'))\n_res = construct_fm(_testdict)\ntest_eq(len(_res.splitlines()), 5)\nprint(_res)\n\n---\ntitle: a title\ndescription: A description\ncategories:  [c1, c2]\n---\n\n\n\n\n\n\ninsert_frontmatter\n\ninsert_frontmatter(nb, fm_dict: dict, filter_keys: list = ['title', 'description', 'author', 'image', 'categories', 'output-file', 'aliases'])\n\nAdd frontmatter into notebook based on filter_keys that exist in fmdict.\n\n\n\n\n\n\n\ninfer_frontmatter\n\ninfer_frontmatter(nb)\n\nInsert front matter if it doesn’t exist automatically from nbdev styled markdown.\n\n\n\n\n_raw_res = _run_procs()\n_res = _run_procs(postprocs=infer_frontmatter)\nassert '# a title' in _raw_res and '# a title' not in _res\nassert r'description: A description\\n' in _res\nassert r'categories:  [c1, c2]\\n' in _res\nassert r'output-file: foobar\\n---' in _res"
  },
  {
    "objectID": "sync.html",
    "href": "sync.html",
    "title": "sync",
    "section": "",
    "text": "nb2dict\n\nnb2dict(d, k=None)\n\nConvert parsed notebook to dict\n\n\n\nThis returns the exact same dict as is read from the notebook JSON.\n\nminimal_fn = Path('../tests/minimal.ipynb')\nminimal = read_nb(minimal_fn)\n\nminimal_dict = minimal_fn.read_json()\nassert minimal_dict==nb2dict(minimal)\n\n\n\n\n\nnb2str\n\nnb2str(nb)\n\nConvert nb to a str\n\n\n\n\n\n\n\nwrite_nb\n\nwrite_nb(nb, path)\n\nWrite nb to path\n\n\n\nThis returns the exact same string as saved by Jupyter.\n\ntmp = Path('tmp.ipynb')\ntry:\n    minimal_txt = minimal_fn.read_text()\n    write_nb(minimal, tmp)\n    assert minimal_txt==tmp.read_text()\nfinally: tmp.unlink()\n\n\n\n\n\nabsolute_import\n\nabsolute_import(name, fname, level)\n\nUnwarps a relative import in name according to fname\n\n\n\n\ntest_eq(absolute_import('xyz', 'nbprocess', 0), 'xyz')\ntest_eq(absolute_import('', 'nbprocess', 1), 'nbprocess')\ntest_eq(absolute_import('core', 'nbprocess', 1), 'nbprocess.core')\ntest_eq(absolute_import('core', 'nbprocess/vision', 2), 'nbprocess.core')\ntest_eq(absolute_import('transform', 'nbprocess/vision', 1), 'nbprocess.vision.transform')\ntest_eq(absolute_import('notebook.core', 'nbprocess/data', 2), 'nbprocess.notebook.core')\n\n\n\n\n\nnbprocess_update\n\nnbprocess_update(fname: str)\n\nPropagates any change in the modules matching fname to the notebooks that created them"
  },
  {
    "objectID": "lookup.html",
    "href": "lookup.html",
    "title": "Doc lookup",
    "section": "",
    "text": "L(pkg_resources.iter_entry_points(group='nbdev'))\n\n(#3) [EntryPoint.parse('nbdev_stdlib = nbdev_stdlib._modidx:d'),EntryPoint.parse('nbdev_numpy = nbdev_numpy._modidx:d'),EntryPoint.parse('nbprocess = nbprocess._modidx:d')]\nSymbol names are taken from libraries registered using the ‘nbprocess’ entry point. By default, all libraries with this entry point are searched, but full symbol names (including module prefix) are required.\nPass strip_libs to list libraries which should be available without requiring a module prefix.\nnbprocess itself includes nbdev_lookup, an instantiated NbdevLookup with strip_libs=nbprocess."
  },
  {
    "objectID": "lookup.html#backticks",
    "href": "lookup.html#backticks",
    "title": "Doc lookup",
    "section": "Backticks",
    "text": "Backticks\n\n\n\n\nNbdevLookup.linkify\n\nNbdevLookup.linkify(self: NbdevLookup, md)\n\n\n\n\n\nmd = \"\"\"This is a link to `numpy.array` and to `read_nb` but not a link to `foobar`.\nAnd not a link to <code>dict2nb</code>.\n\n    This is not a link to `read_nb`\nThis isn’t a link to read_nb either\n\n\n\nc = NbdevLookup('nbprocess')\nMarkdown(c.linkify(md))\n\nThis is a link to numpy.array and to read_nb but not a link to foobar.And not a link to dict2nb. This is not a link to `read_nb`This isn't a link to [read_nb](https://nbprocess.fast.ai/read#read_nb) either"
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Test Notebooks",
    "section": "",
    "text": "nbprocessTestFailure\n\nnbprocessTestFailure(message)\n\nCommon base class for all non-exit exceptions.\n\n\n\n\n\n\n\ntest_nb\n\ntest_nb(fn, skip_flags=None, force_flags=None, do_print=False)\n\nExecute tests in notebook in fn except those with skip_flags\n\n\n\ntest_nb can test a notebook, and skip over certain flags:\n\n_nb = Path('../tests/directives.ipynb')\nsuccess,duration = test_nb(_nb, skip_flags=['notest'])\nassert success\nduration\n\n0.0012979507446289062\n\n\nSometimes you may wish to override one or more of the skip_flags, in which case you can use the argument force_flags which will remove the appropriate tag(s) from skip_flags. This is useful because skip_flags are meant to be set in the tst_flags field of settings.ini, whereas force_flags are usually passed in by the user.\n\n\n\n\nnbprocess_test\n\nnbprocess_test(fname: str = None, flags: str = '', n_workers: int = None, timing: bool = False, do_print: str = False, pause: float = 0.01)\n\nTest in parallel the notebooks matching fname, passing along flags\n\n\n\n\nnbprocess_test(n_workers=0)\n\nSuccess."
  },
  {
    "objectID": "migrate.html",
    "href": "migrate.html",
    "title": "Migrate to nbprocess",
    "section": "",
    "text": "migrate_nb_fm(path, overwrite=True)\n\nMigrate fastpages front matter in notebooks to a raw cell.\n\n\n\n\n_nb = migrate_nb_fm('../tests/2020-09-01-fastcore.ipynb', overwrite=False)\nprint(_get_raw_fm(_nb))\n\n---\ntitle: \"fastcore: An Underrated Python Library\"\ndescription: A unique python library that extends the python programming language and provides utilities that enhance productivity.\nauthor:  \"<a href='https://twitter.com/HamelHusain'>Hamel Husain</a>\"\nimage:  images/copied_from_nb/fastcore_imgs/td.png\ncategories:  [fastcore, fastai]\naliases: [/fastcore/]\n---\n\n\n\n_nb = migrate_nb_fm('../tests/2020-02-20-test.ipynb', overwrite=False)\nprint(_get_raw_fm(_nb))\n\n---\ntitle: Fastpages Notebook Blog Post\ndescription: A tutorial of fastpages for Jupyter notebooks.\nimage:  images/chart-preview.png\ncategories:  [jupyter]\naliases: [/jupyter/2020/02/20/test]\n---\n\n\n\n\n\n\n\n\n\n\n\nmigrate_md_fm(path, overwrite=True)\n\nMake fastpages front matter in markdown files quarto compliant.\n\n\n\nHere is what the front matter of a markdown post looks like before:\n\nprint(run('head -n13 ../tests/2020-01-14-test-markdown-post.md'))\n\n---\ntoc: true\nlayout: post\ndescription: A minimal example of using markdown with fastpages.\ncategories: [markdown]\ntitle: An Example Markdown Post\n---\n# Example Markdown Post\n\n## Basic setup\n\nJekyll requires blog post files to be named according to the following format:\n\n\nAnd this is what it looks like after:\n\n_res = migrate_md_fm('../tests/2020-01-14-test-markdown-post.md', overwrite=False)\nprint(_res[:300])\n\n---\ntitle: An Example Markdown Post\ndescription: A minimal example of using markdown with fastpages.\ncategories: [markdown]\naliases: [/markdown/2020/01/14/test-markdown-post]\n---\n# Example Markdown Post\n\n## Basic setup\n\nJekyll requires blog post files to be named according to the following format:"
  },
  {
    "objectID": "migrate.html#convert-nbdev-v1-projects-to-nbdev-v2",
    "href": "migrate.html#convert-nbdev-v1-projects-to-nbdev-v2",
    "title": "Migrate to nbprocess",
    "section": "Convert nbdev v1 projects to nbdev v2",
    "text": "Convert nbdev v1 projects to nbdev v2\n\nMigrate Directives\n\n\n\n\nnbprocess_migrate_directives\n\nnbprocess_migrate_directives(fname: str = None, disp: bool = False, stdin: bool = False, no_skip: bool = False)\n\nConvert all directives in fname from v1 to v2."
  },
  {
    "objectID": "cli.html",
    "href": "cli.html",
    "title": "cli",
    "section": "",
    "text": "nbprocess_sidebar\n\nnbprocess_sidebar(path: 'str' = None, symlinks: 'bool' = False, file_glob: 'str' = '*.ipynb', file_re: 'str' = None, folder_re: 'str' = None, skip_file_glob: 'str' = None, skip_file_re: 'str' = None, skip_folder_re: 'str' = '^[_.]')\n\nCreate sidebar.yml\n\n\n\n\n\n\n\nFilterDefaults\n\nFilterDefaults()\n\nOverride FilterDefaults to change which notebook processors are used\n\n\n\n\n\n\n\nnbprocess_filter\n\nnbprocess_filter(nb_txt: 'str' = None)\n\nA notebook filter for quarto\n\n\n\n\n\n\n\nnbprocess_bump_version\n\nnbprocess_bump_version(part: 'int' = 2)\n\nIncrement version in settings.py by one\n\n\n\n\n\n\n\nbump_version\n\nbump_version(version, part=2)\n\n\n\n\n\n\n\n\nupdate_version\n\nupdate_version()\n\nAdd or update __version__ in the main __init__.py of the library\n\n\n\n\n\n\n\nextract_tgz\n\nextract_tgz(url, dest='.')\n\n\n\n\n\nos.getenv('GITHUB_TOKEN')\n\n\n\n\n\nprompt_user\n\nprompt_user(**kwargs)\n\n\n\n\n\n\n\n\nrefresh_quarto_yml\n\nrefresh_quarto_yml()\n\nGenerate _quarto.yml from settings.ini.\n\n\n\n\n\n\n\nnbprocess_new\n\nnbprocess_new()\n\nCreate a new project from the current git repo\n\n\n\n\nQuarto\n\n\n\n\nnbprocess_quarto\n\nnbprocess_quarto(path: 'str' = None, doc_path: 'str' = None, symlinks: 'bool' = False, file_glob: 'str' = '*.ipynb', file_re: 'str' = None, folder_re: 'str' = None, skip_file_glob: 'str' = None, skip_file_re: 'str' = None, skip_folder_re: 'str' = '^[_.]')\n\nCreate quarto docs and README.md"
  },
  {
    "objectID": "process.html",
    "href": "process.html",
    "title": "process",
    "section": "",
    "text": "minimal = read_nb('../tests/minimal.ipynb')\n\n\n\n\n\nfirst_code_ln\n\nfirst_code_ln(code_list, re_pattern='\\\\s*#\\\\s*\\\\|')\n\nget first line number where code occurs, where code_list is a list of code\n\n\n\n\n_tst = \"\"\" \n#|default_exp\n #|export\n#|hide_input\nfoo\n\"\"\"\ntest_eq(first_code_ln(_tst.splitlines(True)), 4)\n\n\n\n\n\nextract_directives\n\nextract_directives(cell, remove=True)\n\nTake leading comment directives from lines of code in ss, remove #|, and split\n\n\n\nComment directives start with #|, followed by whitespace delimited tokens, which extract_directives extracts from the start of a cell, up until a blank line or a line containing something other than comments. The extracted lines are removed from the source.\n\nexp  = AttrDict(source = \"\"\"#|export module\n#|eval:false\n#| hide\n# | foo bar\n# |woo: baz\n1+2\n#bar\"\"\")\ntest_eq(extract_directives(exp), {'export':['module'], 'hide':[], 'eval:': ['false'], 'foo': ['bar'], 'woo:': ['baz']})\ntest_eq(exp.source, '#|eval: false\\n# |woo: baz\\n1+2\\n#bar')\n\n\n\n\n\nopt_set\n\nopt_set(var, newval)\n\nnewval if newval else var\n\n\n\n\n\n\n\ninstantiate\n\ninstantiate(x)\n\nInstantiate x if it’s a type\n\n\n\n\n\n\n\nNBProcessor\n\nNBProcessor(path=None, procs=None, preprocs=None, postprocs=None, nb=None, debug=False, rm_directives=True)\n\nProcess cells and nbdev comments in a notebook\n\n\n\nCell processors can be callables (e.g regular functions), in which case they are called for every cell:\n\neverything_fn = '../tests/01_everything.ipynb'\n\ndef print_execs(cell):\n    if 'exec' in cell.source: print(cell.source)\n\nNBProcessor(everything_fn, print_execs).process()\n\nexec(\"o_y=1\")\nexec(\"p_y=1\")\n_all_ = [o_y, 'p_y']\n\n\nComment directives are put in a cell attribute directive_ as a dictionary keyed by directive name:\n\ndef printme_func(cell):\n    if cell.directives_ and 'printme' in cell.directives_: print(cell.directives_['printme'])\n\nNBProcessor(everything_fn, printme_func).process()\n\n['testing']\n\n\nHowever, a more convenient way to handle comment directives is to use a class as a processor, and include a method in your class with the same name as your directive, surrounded by underscores:\n\nclass _PrintExample:\n    def _printme_(self, nbp, cell, to_print): print(to_print)\n\nNBProcessor(everything_fn, _PrintExample()).process()\n\ntesting\n\n\nIn the case that your processor supports just one comment directive, you can just use a regular function, with the same name as your directive, but with an underscore appended – here printme_ is identical to _PrintExample above:\n\ndef printme_(nbp, cell, to_print): print(to_print)\n\nNBProcessor(everything_fn, printme_).process()\n\ntesting\n\n\n\nbasic_export_nb2('01_read.ipynb', 'read')\nbasic_export_nb2('02_maker.ipynb', 'maker')\nbasic_export_nb2('03_process.ipynb', 'process')\n\ng = exec_new('import nbprocess.process')\nassert hasattr(g['nbprocess'].process, 'NBProcessor')"
  },
  {
    "objectID": "11_clean.html",
    "href": "11_clean.html",
    "title": "Clean",
    "section": "",
    "text": "To avoid pointless conflicts while working with jupyter notebooks (with different execution counts or cell metadata), it is recommended to clean the notebooks before committing anything (done automatically if you install the git hooks with nbprocess_install_git_hooks). The following functions are used to do that."
  },
  {
    "objectID": "11_clean.html#utils",
    "href": "11_clean.html#utils",
    "title": "Clean",
    "section": "Utils",
    "text": "Utils\n\n\n\n\nclean_nb\n\nclean_nb(nb, clear_all=False)\n\nClean nb from superfluous metadata\n\n\n\n\ntst = {'cell_type': 'code', 'execution_count': 26,\n       'metadata': {'hide_input': True, 'meta': 23},\n       'outputs': [{'execution_count': 2,\n                    'data': {\n                        'application/vnd.google.colaboratory.intrinsic+json': {'type': 'string'},\n                        'plain/text': ['sample output',]\n                    }, 'output': 'super'}],\n       'source': 'awesome_code'}\nnb = {'metadata': {'kernelspec': 'some_spec', 'jekyll': 'some_meta', 'meta': 37}, 'cells': [tst]}\n\nclean_nb(nb)\ntest_eq(nb['cells'][0], {'cell_type': 'code', 'execution_count': None,\n              'metadata': {'hide_input': True},\n              'outputs': [{'execution_count': None, \n                           'data': { 'plain/text': ['sample output',]},\n                           'output': 'super'}],\n              'source': 'awesome_code'})\ntest_eq(nb['metadata'], {'kernelspec': 'some_spec', 'jekyll': 'some_meta'})\n\n\n\n\n\nwrapio\n\nwrapio(strm)\n\n\n\n\n\n\n\n\nprocess_write\n\nprocess_write(warn_msg, proc_nb, f_in, f_out=None, disp=False)\n\n\n\n\n\n\n\n\nnbprocess_clean\n\nnbprocess_clean(fname: str = None, clear_all: bool = False, disp: bool = False, stdin: bool = False)\n\nClean all notebooks in fname to avoid merge conflicts\n\n\n\nBy default (fname left to None), the all the notebooks in lib_folder are cleaned. You can opt in to fully clean the notebook by removing every bit of metadata and the cell outputs by passing clear_all=True.\n\n\n\n\nnbprocess_install_hooks\n\nnbprocess_install_hooks()\n\nInstall git hooks to clean/trust notebooks automatically"
  },
  {
    "objectID": "doclinks.html",
    "href": "doclinks.html",
    "title": "doclinks",
    "section": "",
    "text": "The doc index has to be stored in a file. Usually we call it _modidx.py. For testing, we’ll delete any existing file first.\n\ndest_fn = Path('tmp/_modidx.py')\nwith contextlib.suppress(FileNotFoundError): dest_fn.unlink()\n\nA link to docs is created by a doc_func. We’ll use a dummy one for testing.\n\ndef _help(m, s=None): return f\"help for {m}; {s}\"\n\nWe’re now ready to instantiate DocLinks for our test module.\n\nmod_fn = Path('tmp/everything.py')\nlink = DocLinks(mod_fn, _help, dest_fn)\nlink.mod_name\n\n'tmp.everything'\n\n\n\n\n\n\nDocLinks.write_nbprocess_idx\n\nDocLinks.write_nbprocess_idx(self: DocLinks)\n\nCreate nbprocess documentation index file`\n\n\n\nInitially the index file will contain empty syms and settings:\n\ntmp_path = Path('tmp')\ntmp_path.mkdir(exist_ok=True)\nlink.write_nbprocess_idx()\nassert \"Autogenerated\" in dest_fn.read_text()\n\nprint(Path('tmp/_modidx.py').read_text())\n\n# Autogenerated by nbprocess\n\nd = {'settings': {}, 'syms': {}}\n\n\n\n\n\n\nget_patch_name\n\nget_patch_name(o)\n\n\n\n\n\ns = \"\"\"class _T: pass\n@patch\ndef _f(self:_T): pass\n@patch_to(_T)\ndef _g(self): pass\"\"\"\n\nres = [get_patch_name(o) for o in ast.parse(s).body]\ntest_eq([None, '_T._f', '_T._g'], res)\n\n\n\n\n\nDocLinks.update_syms\n\nDocLinks.update_syms(self: DocLinks)\n\n\n\n\n\neverything_fn = '../tests/01_everything.ipynb'\nnb_export('../tests/00_some.thing.ipynb', 'tmp')\nnb_export(everything_fn, 'tmp')\n\n\nlink.update_syms()\nlink.write_nbprocess_idx()\n\n\ng = exec_new('import tmp._modidx')\nd = g['tmp']._modidx.d\nsymn = 'tmp.everything.a_y'\nmod_name = 'tmp.everything'\ntest_eq(d['syms'][mod_name][symn], _help(mod_name,symn))\ntest_eq(set(d['syms'][mod_name].keys()),\n        set(L('m_y', 'n_y', 'q_y', 'a_y', 'b_y', 'd_y', 'e_y', 'o_y', 'p_y', 'd_y.di_n', 'd_y.d3i_n', 'd_y.d4i_n'\n             ).map('tmp.everything.{}')))\n\n\n\n\n\nDocLinks.build_index\n\nDocLinks.build_index(self: DocLinks)\n\n\n\n\n\nlink.build_index()\ndel(sys.modules['tmp._modidx'])\ng = exec_new('import tmp._modidx')\nd = g['tmp']._modidx.d\ntest_eq(d['settings']['lib_name'], 'nbprocess')\n\n\n\n\n\nbuild_modidx\n\nbuild_modidx()\n\nCreate _modidx.py\n\n\n\n\n\n\n\nnbglob\n\nnbglob(path=None, recursive=True, symlinks=True, file_glob='*.ipynb', file_re=None, folder_re=None, skip_file_glob=None, skip_file_re=None, skip_folder_re='^[_.]', key='nbs_path')\n\nFind all files in a directory matching an extension given a config_key.\n\n\n\n\n\n\n\nnbprocess_export\n\nnbprocess_export(path: str = None, recursive: bool = True, symlinks: bool = True, file_glob: str = '*.ipynb', file_re: str = None, folder_re: str = None, skip_file_glob: str = None, skip_file_re: str = None, skip_folder_re: str = '^[_.]')\n\nExport notebooks in path to python modules\n\n\n\n\nPath('../nbprocess/export.py').unlink(missing_ok=True)\nnbprocess_export()\n\ng = exec_new('import nbprocess.export')\nassert hasattr(g['nbprocess'].export, 'nb_export')\nfrom nbprocess._modidx import d\nassert d['syms']['nbprocess.doclinks']['nbprocess.doclinks.DocLinks'].startswith('http')"
  },
  {
    "objectID": "read.html",
    "href": "read.html",
    "title": "read",
    "section": "",
    "text": "A notebook is just a json file.\n\nminimal_fn = Path('../tests/minimal.ipynb')\nminimal_txt = AttrDict(minimal_fn.read_json())\n\nIt contains two sections, the metadata…:\n\nminimal_txt.metadata\n\n{'kernelspec': {'display_name': 'Python 3 (ipykernel)',\n  'language': 'python',\n  'name': 'python3'},\n 'language_info': {'codemirror_mode': {'name': 'ipython', 'version': 3},\n  'file_extension': '.py',\n  'mimetype': 'text/x-python',\n  'name': 'python',\n  'nbconvert_exporter': 'python',\n  'pygments_lexer': 'ipython3',\n  'version': '3.9.7'}}\n\n\n…and, more importantly, the cells:\n\nminimal_txt.cells\n\n[{'cell_type': 'markdown',\n  'metadata': {},\n  'source': ['## A minimal notebook']},\n {'cell_type': 'code',\n  'execution_count': 1,\n  'metadata': {},\n  'outputs': [{'data': {'text/plain': ['2']},\n    'execution_count': 1,\n    'metadata': {},\n    'output_type': 'execute_result'}],\n  'source': ['# Do some arithmetic\\n', '1+1']}]\n\n\nThe second cell here is a code cell, however it contains no outputs, because it hasn’t been executed yet. To execute a notebook, we first need to convert it into a format suitable for nbclient (which expects some dict keys to be available as attrs, and some available as regular dict keys). Normally, nbformat is used for this step, but it’s rather slow and inflexible, so we’ll write our own function based on fastcore’s handy dict2obj, which makes all keys available as both attrs and keys.\n\n\n\n\n\n\nNbCell(idx, cell)\n\ndict subclass that also provides access to keys as attrs\n\n\n\nWe use an AttrDict subclass which has some basic functionality for accessing notebook cells.\n\n\n\n\n\n\ndict2nb(js)\n\nConvert dict js to an AttrDict,\n\n\n\nWe can now convert our JSON into this nbclient-compatible format, which pretty prints the source code of cells in notebooks.\n\nminimal = dict2nb(minimal_txt)\ncell = minimal.cells[1]\ncell\n\npython# Do some arithmetic1+1\n\n\nThe abstract syntax tree of source code cells is available in the parsed_ property:\n\ncell.parsed_(), cell.parsed_()[0].value.op\n\n([<ast.Expr at 0x7f8160e4e790>], <ast.Add at 0x7f81300cca30>)\n\n\nSince loading JSON and converting to an NB is something we’ll do a lot, we’ll create a shortcut function for it:\n\n\n\n\n\n\nread_nb(path)\n\nReturn notebook at path\n\n\n\n\nminimal = read_nb(minimal_fn)\nstr(minimal.cells[0])\n\n\"{'cell_type': 'markdown', 'metadata': {}, 'source': '## A minimal notebook', 'idx_': 0}\""
  },
  {
    "objectID": "read.html#executing-a-notebook",
    "href": "read.html#executing-a-notebook",
    "title": "read",
    "section": "Executing a notebook",
    "text": "Executing a notebook\nInstead of using nbclient, which is quite slow and requires installing jupyter, NBRunner is a subclass of TinyKernel which can run notebook cells and store their results, like nbclient does. We also define some simple helper functions here that NBRunner needs.\n\n\n\n\nmk_cell\n\nmk_cell(text, code=True)\n\nCreate a NbCell containing text\n\n\n\n\n\n\n\ncreate_output\n\ncreate_output(txt, mime)\n\nAdd a cell output containing txt of the mime text MIME sub-type\n\n\n\n\n\n\n\nNBRunner\n\nNBRunner(name='kernel', glb=None)\n\nA TinyKernel subclass that adds a run method to execute notebook cells\n\n\n\nWe can now execute the notebook, and see that the cell has been executed, with the output added back to the notebook:\n\nNBRunner().exec_nb(minimal)\ncell.outputs[0]\n\njson{ 'data': {'text/plain': ['2']},  'execution_count': 1,  'metadata': {},  'output_type': 'execute_result'}"
  },
  {
    "objectID": "read.html#config",
    "href": "read.html#config",
    "title": "read",
    "section": "Config",
    "text": "Config\nnbprocess uses a settings.ini file in the root of the project to store all configuration details. This file is in ConfigParser format, and can be read and written conveniently using fastcore’s Config class.\n\n\n\n\nnbprocess_create_config\n\nnbprocess_create_config(user: str, lib_name: str = None, description='TODO fill me in', author='TODO fill me in', author_email='todo@example.org', path: str = '.', cfg_name: str = 'settings.ini', branch: str = 'master', host: str = 'github', git_url: str = 'https://github.com/%(user)s/%(lib_name)s/tree/%(branch)s/', custom_sidebar: <function bool_arg at 0x7f325ae860d0> = False, nbs_path: str = '.', lib_path: str = '%(lib_name)s', doc_path: str = 'docs', tst_flags: str = '', version: str = '0.0.1', keywords='python', license='apache2', copyright='', status='3', min_python='3.6', audience='Developers', language='English')\n\nCreates a new config file for lib_name and user and saves it.\n\n\n\nThis is a wrapper for fastcore’s save_config_file which sets some nbprocess defaults. It is also installed as a CLI command.\n\n\n\n\nget_config\n\nget_config(cfg_name='settings.ini', path=None)\n\nConfig for ini file found in path (defaults to cwd)\n\n\n\nget_config searches for settings.ini in the current directory, and then in all parent directories, stopping when it is found.\n\nnbprocess_create_config('fastai', path='..', nbs_path='nbs', tst_flags='tst', cfg_name='test_settings.ini')\ncfg = get_config('test_settings.ini')\ntest_eq(cfg.lib_name, 'nbprocess')\ntest_eq(cfg.git_url, \"https://github.com/fastai/nbprocess/tree/master/\")\ncwd = Path.cwd()\ntest_eq(cfg.config_path, cwd.parent.absolute())\ntest_eq(cfg.path('lib_path'), cwd.parent/'nbprocess')\ntest_eq(cfg.path('nbs_path'), cwd)\ntest_eq(cfg.path('doc_path'), cwd.parent/'docs')\n\n\n\n\n\nconfig_key\n\nconfig_key(c, default=None, path=True, missing_ok=False)\n\nLook for key c in settings.ini and fail gracefully if not found and no default provided"
  },
  {
    "objectID": "read.html#exporting-a-basic-module",
    "href": "read.html#exporting-a-basic-module",
    "title": "read",
    "section": "Exporting a basic module",
    "text": "Exporting a basic module\n\n\n\n\nadd_init\n\nadd_init(path)\n\nAdd __init__.py in all subdirs of path containing python files if it’s not there already\n\n\n\nPython modules require a __init.py__ file in all directories that are modules. We assume that all directories containing a python file (including in subdirectories of any depth) is a module, and therefore add a __init__.py to each.\n\nwith tempfile.TemporaryDirectory() as d:\n    d = Path(d)\n    (d/'a/b').mkdir(parents=True)\n    (d/'a/b/f.py').touch()\n    (d/'a/c').mkdir()\n    add_init(d)\n    assert not (d/'a/c'/_init).exists(), \"Should not add init to dir without py file\"\n    for e in [d, d/'a', d/'a/b']: assert (e/_init).exists(),f\"Missing init in {e}\"\n\n\n\n\n\nwrite_cells\n\nwrite_cells(cells, hdr, file, offset=0)\n\nWrite cells to file along with header hdr starting at index offset (mainly for nbprocess internal use)\n\n\n\n\n\n\n\nbasic_export_nb\n\nbasic_export_nb(fname, name, dest=None)\n\nBasic exporter to bootstrap nbprocess\n\n\n\nThis is a simple exporter with just enough functionality to correctly export this notebook, in order to bootstrap the creation of nbprocess itself."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "nbprocess",
    "section": "",
    "text": "This will probably become v2 of nbdev in the near-ish future."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "nbprocess",
    "section": "Install",
    "text": "Install\nWith pip:\npip install nbprocess\nWith conda:\nconda install -c fastai nbprocess"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "nbprocess",
    "section": "How to use",
    "text": "How to use\nBy default docs are exported for use with Quarto. To install Quarto on Ubuntu, run make install. See the Quarto docs for other platforms.\nThe following CLI tools are provided:\n\nnbprocess_create_config: Create settings.ini skeleton\nnbprocess_export: Export notebooks to Python modules\nnbprocess_update: Update Python modules from a notebook\nnbprocess_fix: Fix merge conflicts in notebooks\nnbprocess_filter: A filter for Quarto\nnbprocess_quarto: Create Quarto web site\nnbprocess_new: Create a new nbprocess project\nnbprocess_migrate_directives: helps you migrate all your directives from nbdev v1 to v2."
  },
  {
    "objectID": "maker.html",
    "href": "maker.html",
    "title": "maker",
    "section": "",
    "text": "These functions let us find and modify the definitions of variables in python modules.\n\n\n\n\n\n\nfind_var(lines, varname)\n\nFind the line numbers where varname is defined in lines\n\n\n\n\nt = '''a_=(1,\n  2,\n  3)\n\nb_=3'''\ntest_eq(find_var(t.splitlines(), 'a_'), (0,3))\ntest_eq(find_var(t.splitlines(), 'b_'), (4,5))\n\n\n\n\n\n\n\nread_var(code, varname)\n\nEval and return the value of varname defined in code\n\n\n\n\ntest_eq(read_var(t, 'a_'), (1,2,3))\ntest_eq(read_var(t, 'b_'), 3)\n\n\n\n\n\n\n\nupdate_var(varname, func, fn=None, code=None)\n\nUpdate the definition of varname in file fn, by calling func with the current definition\n\n\n\n\ng = exec_new(t)\ntest_eq((g['a_'],g['b_']), ((1,2,3),3))\nt2 = update_var('a_', lambda o:0, code=t)\nexec(t2, g)\ntest_eq((g['a_'],g['b_']), (0,3))\nt3 = update_var('b_', lambda o:0, code=t)\nexec(t3, g)\ntest_eq((g['a_'],g['b_']), ((1,2,3),0))\n\n\n\n\n\n\n\nModuleMaker(dest, name, nb_path, is_new=True)\n\nHelper class to create exported library from notebook source cells\n\n\n\nIn order to export a notebook, we need an way to create a Python file. ModuleMaker fills that role. Pass in the directory where you want to module created, the name of the module, the path of the notebook source, and set is_new to True if this is a new file being created (rather than an existing file being added to). The location of the saved module will be in fname.\n\nmm = ModuleMaker(dest='tmp', name='test.testing', nb_path=Path.cwd()/'01_export.ipynb', is_new=True)\nmm.fname\n\nPath('tmp/test/testing.py')\n\n\n\n\n\n\n\n\ndecor_id(d)\n\nid attr of decorator, regardless of whether called as function or bare\n\n\n\n\n\n\n\n\n\nretr_exports(trees)\n\n\n\n\n\n\n\n\n\n\nModuleMaker.make_all(self: ModuleMaker, cells)\n\nCreate __all__ with all exports in cells\n\n\n\n\n\n\n\n\n\nmake_code_cells(*ss)\n\n\n\n\n\n\n\n\n\n\nmake_code_cell(code)\n\n\n\n\nWe want to add an __all__ to the top of the exported module. This methods autogenerates it from all code in cells.\n\nnb = make_code_cells(\"from __future__ import print_function\", \"def a():...\", \"def b():...\",\n                      \"c=d=1\", \"_f=1\", \"_g=1\", \"_all_=['_g']\", \"@patch\\ndef h(self:ca):...\")\ntest_eq(set(mm.make_all(nb)), set(['a','b','c','d', '_g']))\n\n\n\n\n\n\n\nrelative_import(name, fname, level=0)\n\nConvert a module name to a name relative to fname\n\n\n\n\ntest_eq(relative_import('nbprocess.core', \"xyz\"), 'nbprocess.core')\ntest_eq(relative_import('nbprocess.core', 'nbprocess'), '.core')\n_p = Path('fastai')\ntest_eq(relative_import('fastai.core', _p/'vision'), '..core')\ntest_eq(relative_import('fastai.core', _p/'vision/transform'), '...core')\ntest_eq(relative_import('fastai.vision.transform', _p/'vision'), '.transform')\ntest_eq(relative_import('fastai.notebook.core', _p/'data'), '..notebook.core')\ntest_eq(relative_import('fastai.vision', _p/'vision'), '.')\n\n\n\n\n\n\n\nNbCell.import2relative(cell: nbprocess.read.NbCell, libname)\n\n\n\n\n\n\n\n\n\n\nupdate_import(source, tree, libname, f=<function relative_import at 0x7fd4e0c6a550>)\n\n\n\n\n\nss = \"from nbprocess.export import *\\nfrom nbprocess.a.b import *\"\ncell = make_code_cells([ss])[0]\ncell.import2relative('nbprocess')\ntest_eq(cell.source, 'from .export import *\\nfrom .a.b import *')\n\ncell = make_code_cells([ss])[0]\ncell.import2relative('nbprocess/a')\ntest_eq(cell.source, 'from ..export import *\\nfrom .b import *')\n\n\n\n\n\n\n\nModuleMaker.make(self: ModuleMaker, cells, all_cells=None, lib_name=None)\n\nWrite module containing cells with __all__ generated from all_cells\n\n\n\n\ndef _print_file(fname, mx=None): print(Path(fname).read_text().strip()[:ifnone(mx,9999)])\n\n\ncells = make_code_cells(\"from __future__ import print_function\", \"#|export\\ndef a(): ...\", \"def b(): ...\")\nmm.make(cells, L([cells[1]]))\nprint(Path('tmp/test/testing.py').read_text())\n\n# AUTOGENERATED! DO NOT EDIT! File to edit: ../01_export.ipynb.\n\n# %% ../01_export.ipynb 0\nfrom __future__ import print_function\n\n# %% auto 0\n__all__ = ['a']\n\n# %% ../01_export.ipynb 2\n#|export\ndef a(): ...\n\n# %% ../01_export.ipynb 3\ndef b(): ...\n\n\n\nPass all_cells=[] if you don’t want any __all__ added.\nIf is_new=False then the additional definitions are added to the bottom, and any existing __all__ is updated with the newly-added symbols.\n\nc2 = make_code_cells(\"def c(): ...\", \"def d(): ...\")\nmm = ModuleMaker(dest='tmp', name='test.testing', nb_path=Path.cwd()/'01_export.ipynb', is_new=False)\nmm.make(c2, c2)\n\n\ng = exec_import('.tmp.test.testing', '*')\nfor s in \"a c d\".split(): assert s in g, s\nassert 'b' not in g\nassert g['a']() is None\n\n\n\n\n\n\n\nbasic_export_nb2(fname, name, dest=None)\n\nA basic exporter to bootstrap nbprocess using ModuleMaker\n\n\n\n\npath = Path('../nbprocess')\n(path/'read.py').unlink(missing_ok=True)\n(path/'maker.py').unlink(missing_ok=True)\n\nadd_init(path)\ncfg = get_config()\n\nbasic_export_nb2('01_read.ipynb', 'read')\nbasic_export_nb2('02_maker.ipynb', 'maker')\n\ng = exec_import('nbprocess', 'maker')\nassert g['maker'].ModuleMaker\nassert 'ModuleMaker' in g['maker'].__all__"
  },
  {
    "objectID": "merge.html",
    "href": "merge.html",
    "title": "merge",
    "section": "",
    "text": "When working with jupyter notebooks (which are json files behind the scenes) and GitHub, it is very common that a merge conflict (that will add new lines in the notebook source file) will break some notebooks you are working on. This module defines the function fix_conflicts to fix those notebooks for you, and attempt to automatically merge standard conflicts. The remaining ones will be delimited by markdown cells like this:\n<<<<<< HEAD\n\n# local code here\n\n======\n\n# remote code here\n\n>>>>>> a7ec1b0bfb8e23b05fd0a2e6cafcb41cd0fb1c35\nBelow is an example of broken notebook. The json format is broken by the lines automatically added by git. Such a file can’t be opened in jupyter notebook.\n\nbroken = Path('../tests/example.ipynb.broken')\ntst_nb = broken.read_text()\nprint(tst_nb)\n\n{\n \"cells\": [\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 6,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/plain\": [\n       \"3\"\n      ]\n     },\n     \"execution_count\": 6,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n<<<<<<< HEAD\n    \"z=3\\n\",\n=======\n    \"z=2\\n\",\n>>>>>>> a7ec1b0bfb8e23b05fd0a2e6cafcb41cd0fb1c35\n    \"z\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 7,\n   \"execution_count\": 5,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/plain\": [\n       \"6\"\n      ]\n     },\n<<<<<<< HEAD\n     \"execution_count\": 7,\n=======\n     \"execution_count\": 5,\n>>>>>>> a7ec1b0bfb8e23b05fd0a2e6cafcb41cd0fb1c35\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"x=3\\n\",\n    \"y=3\\n\",\n    \"x+y\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": []\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 2\n}\n\n\n\nNote that in this example, the second conflict is easily solved: it just concerns the execution count of the second cell and can be solved by choosing either option without really impacting your notebook. This is the kind of conflict we will fix automatically. The first conflict is more complicated as it spans across two cells and there is a cell present in one version, not the other. Such a conflict (and generally the ones where the inputs of the cells change form one version to the other) aren’t automatically fixed, but we will return a proper json file where the annotations introduced by git will be placed in markdown cells."
  },
  {
    "objectID": "merge.html#creating-a-merged-notebook",
    "href": "merge.html#creating-a-merged-notebook",
    "title": "merge",
    "section": "Creating a merged notebook",
    "text": "Creating a merged notebook\nThe approach we use is to first “unpatch” the conflicted file, regenerating the two files it was originally created from. Then we redo the diff process, but using cells instead of text lines.\n\n\n\n\nunpatch\n\nunpatch(s: str)\n\nTakes a string with conflict markers and returns the two original files, and their branch names\n\n\n\nThe result of “unpatching” our conflicted test notebook is the two original notebooks it would have been created from. Each of these original notebooks will contain valid JSON:\n\na,b,branch1,branch2 = unpatch(tst_nb)\njson.loads(a)\n\n{'cells': [{'cell_type': 'code',\n   'execution_count': 6,\n   'metadata': {},\n   'outputs': [{'data': {'text/plain': ['3']},\n     'execution_count': 6,\n     'metadata': {},\n     'output_type': 'execute_result'}],\n   'source': ['z=3\\n', 'z']},\n  {'cell_type': 'code',\n   'execution_count': 5,\n   'metadata': {},\n   'outputs': [{'data': {'text/plain': ['6']},\n     'execution_count': 7,\n     'metadata': {},\n     'output_type': 'execute_result'}],\n   'source': ['x=3\\n', 'y=3\\n', 'x+y']},\n  {'cell_type': 'code',\n   'execution_count': None,\n   'metadata': {},\n   'outputs': [],\n   'source': []}],\n 'metadata': {'kernelspec': {'display_name': 'Python 3',\n   'language': 'python',\n   'name': 'python3'}},\n 'nbformat': 4,\n 'nbformat_minor': 2}\n\n\n\njson.loads(b)\n\n{'cells': [{'cell_type': 'code',\n   'execution_count': 6,\n   'metadata': {},\n   'outputs': [{'data': {'text/plain': ['3']},\n     'execution_count': 6,\n     'metadata': {},\n     'output_type': 'execute_result'}],\n   'source': ['z=2\\n', 'z']},\n  {'cell_type': 'code',\n   'execution_count': 5,\n   'metadata': {},\n   'outputs': [{'data': {'text/plain': ['6']},\n     'execution_count': 5,\n     'metadata': {},\n     'output_type': 'execute_result'}],\n   'source': ['x=3\\n', 'y=3\\n', 'x+y']},\n  {'cell_type': 'code',\n   'execution_count': None,\n   'metadata': {},\n   'outputs': [],\n   'source': []}],\n 'metadata': {'kernelspec': {'display_name': 'Python 3',\n   'language': 'python',\n   'name': 'python3'}},\n 'nbformat': 4,\n 'nbformat_minor': 2}\n\n\n\nbranch1,branch2\n\n('HEAD', 'a7ec1b0bfb8e23b05fd0a2e6cafcb41cd0fb1c35')\n\n\n\n\n\n\nnbprocess_fix\n\nnbprocess_fix(nbname: str, outname: str = None, nobackup: bool = True, theirs: bool = False, noprint: bool = False)\n\nCreate working notebook from conflicted notebook nbname\n\n\n\nThis begins by optionally backing the notebook fname to fname.bak in case something goes wrong. Then it parses the broken json, solving conflicts in cells. Every conflict that only involves metadata or outputs of cells will be solved automatically by using the local (theirs==False) or the remote (theirs==True) branch. Otherwise, or for conflicts involving the inputs of cells, the json will be repaired by including the two version of the conflicted cell(s) with markdown cells indicating the conflicts. You will be able to open the notebook again and search for the conflicts (look for <<<<<<<) then fix them as you wish.\nA message will be printed indicating whether the notebook was fully merged or if conflicts remain.\n\nnbprocess_fix(broken, outname='tmp.ipynb')\nchk = read_nb('tmp.ipynb')\ntest_eq(len(chk.cells), 7)\nos.unlink('tmp.ipynb')\n\nOne or more conflict remains in the notebook, please inspect manually."
  },
  {
    "objectID": "export.html",
    "href": "export.html",
    "title": "export",
    "section": "",
    "text": "Specify dest where the module(s) will be exported to, and optionally a class to use to create the module (ModuleMaker, by default).\nExported cells are stored in a dict called modules, where the keys are the modules exported to. Those without an explicit module are stored in the '#' key, which will be exported to default_exp.\n\neverything_fn = '../tests/01_everything.ipynb'\n\nexp = ExportModuleProc()\nproc = NBProcessor(everything_fn, exp)\nproc.process()\ntest_eq(exp.default_exp, 'everything')\nassert 'print_function'  in exp.modules['#'][0].source\nassert 'h_n' in exp.in_all['some.thing'][0].source\n\n\n\n\n\ncreate_modules\n\ncreate_modules(path, dest, procs=None, debug=False, mod_maker=<class 'nbprocess.maker.ModuleMaker'>)\n\nCreate module(s) from notebook\n\n\n\nLet’s check we can import a test file:\n\nshutil.rmtree('tmp', ignore_errors=True)\ncreate_modules('../tests/00_some.thing.ipynb', 'tmp')\n\ng = exec_new('import tmp.some.thing')\ntest_eq(g['tmp'].some.thing.__all__, ['a'])\ntest_eq(g['tmp'].some.thing.a, 1)\n\nWe’ll also check that our ‘everything’ file exports correctly:\n\ncreate_modules(everything_fn, 'tmp')\n\ng = exec_new('import tmp.everything; from tmp.everything import *')\n_alls = L(\"a b d e m n o p q\".split())\nfor s in _alls.map(\"{}_y\"): assert s in g, s\nfor s in \"c_y_nall _f_y_nall g_n h_n i_n j_n k_n l_n\".split(): assert s not in g, s\nfor s in _alls.map(\"{}_y\") + [\"c_y_nall\", \"_f_y_nall\"]: assert hasattr(g['tmp'].everything,s), s\n\nThat notebook should also export one extra function to tmp.some.thing:\n\ndel(sys.modules['tmp.some.thing']) # remove from module cache\ng = exec_new('import tmp.some.thing')\ntest_eq(g['tmp'].some.thing.__all__, ['a','h_n'])\ntest_eq(g['tmp'].some.thing.h_n(), None)\n\n\n\n\n\nnb_export\n\nnb_export(nbname, lib_path=None)\n\n\n\n\n\nPath('../nbprocess/export.py').unlink(missing_ok=True)\nnb_export('04a_export.ipynb')\n\ng = exec_new('import nbprocess.export')\nassert hasattr(g['nbprocess'].export, 'nb_export')"
  }
]